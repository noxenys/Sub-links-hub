# è‡ªåŠ¨åŒ–çˆ¬è™«ç³»ç»Ÿéƒ¨ç½²æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—æä¾›äº†ä¸‰ç§éƒ¨ç½²æ–¹å¼æ¥è¿è¡Œ SubLinks Hub çš„è‡ªåŠ¨åŒ–çˆ¬è™«ç³»ç»Ÿï¼Œç”¨äºå®šæœŸä» GitHubã€Telegramã€è®ºå›ç­‰å¤šä¸ªæ¥æºæŠ“å–è®¢é˜…é“¾æ¥ã€‚

---

## ğŸš€ éƒ¨ç½²æ–¹å¼ 1ï¼šManus å®šæ—¶ä»»åŠ¡ï¼ˆæ¨èï¼‰

### ä¼˜ç‚¹
- âœ… ä¸åº”ç”¨ç´§å¯†é›†æˆ
- âœ… æ— éœ€é¢å¤–æœåŠ¡
- âœ… è‡ªåŠ¨æ‰©å±•å’Œå¤‡ä»½
- âœ… å†…ç½®ç›‘æ§å’Œå‘Šè­¦

### å®ç°æ­¥éª¤

#### 1. åˆ›å»ºçˆ¬è™«è„šæœ¬

åœ¨ `server/cron/crawler.ts` ä¸­åˆ›å»ºçˆ¬è™«è„šæœ¬ï¼š

```typescript
import cron from 'node-cron';
import axios from 'axios';

// çˆ¬è™«é…ç½®
const CRAWLER_CONFIG = {
  github: {
    enabled: true,
    interval: '0 */6 * * *', // æ¯ 6 å°æ—¶æ‰§è¡Œä¸€æ¬¡
    maxStars: 100,
    maxForks: 50,
  },
  forum: {
    enabled: true,
    interval: '0 */12 * * *', // æ¯ 12 å°æ—¶æ‰§è¡Œä¸€æ¬¡
  },
  validation: {
    enabled: true,
    interval: '0 0 * * 0', // æ¯å‘¨æ—¥åˆå¤œæ‰§è¡Œ
  },
};

/**
 * GitHub çˆ¬è™«
 */
async function crawlGitHub() {
  try {
    console.log('[GitHub Crawler] Starting...');

    const keywords = [
      'subscription',
      'proxy',
      'clash',
      'v2ray',
      'trojan',
      'vless',
      'vmess',
    ];

    for (const keyword of keywords) {
      const query = `${keyword} in:readme stars:<${CRAWLER_CONFIG.github.maxStars} forks:<${CRAWLER_CONFIG.github.maxForks}`;
      const url = `https://api.github.com/search/repositories?q=${encodeURIComponent(query)}&sort=updated&order=desc&per_page=30`;

      const response = await axios.get(url, {
        timeout: 15000,
        headers: {
          'User-Agent': 'SubLinksHub-Crawler/1.0',
        },
      });

      const repos = response.data.items || [];

      for (const repo of repos) {
        // æå–é“¾æ¥é€»è¾‘
        console.log(`[GitHub Crawler] Processing ${repo.full_name}`);

        // ä» README æå–é“¾æ¥
        try {
          const readmeUrl = `https://raw.githubusercontent.com/${repo.full_name}/main/README.md`;
          const readmeResponse = await axios.get(readmeUrl, { timeout: 10000 });
          const readmeContent = readmeResponse.data;

          // ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–é“¾æ¥
          const linkRegex = /(https?:\/\/[^\s]+|vmess:\/\/[^\s]+|vless:\/\/[^\s]+|trojan:\/\/[^\s]+|ss:\/\/[^\s]+|ssr:\/\/[^\s]+)/gi;
          const links = readmeContent.match(linkRegex) || [];

          for (const link of links) {
            // ä¿å­˜åˆ°æ•°æ®åº“
            console.log(`[GitHub Crawler] Found link: ${link}`);
            // await saveLink(link, 'github', repo.html_url, repo.full_name);
          }
        } catch (error) {
          console.error(`[GitHub Crawler] Error processing ${repo.full_name}:`, error);
        }
      }

      // é¿å… API é™åˆ¶
      await new Promise(resolve => setTimeout(resolve, 1000));
    }

    console.log('[GitHub Crawler] Completed');
  } catch (error) {
    console.error('[GitHub Crawler] Error:', error);
  }
}

/**
 * è®ºå›çˆ¬è™«
 */
async function crawlForums() {
  try {
    console.log('[Forum Crawler] Starting...');

    // å®ç°è®ºå›çˆ¬è™«é€»è¾‘
    // æŠ“å– V2EXã€HostLoc ç­‰è®ºå›ä¸­çš„é“¾æ¥

    console.log('[Forum Crawler] Completed');
  } catch (error) {
    console.error('[Forum Crawler] Error:', error);
  }
}

/**
 * é“¾æ¥éªŒè¯
 */
async function validateLinks() {
  try {
    console.log('[Link Validator] Starting...');

    // å®ç°é“¾æ¥éªŒè¯é€»è¾‘
    // æ£€æŸ¥æ‰€æœ‰é“¾æ¥çš„å¯ç”¨æ€§

    console.log('[Link Validator] Completed');
  } catch (error) {
    console.error('[Link Validator] Error:', error);
  }
}

/**
 * åˆå§‹åŒ–å®šæ—¶ä»»åŠ¡
 */
export function initializeCrawlerTasks() {
  if (CRAWLER_CONFIG.github.enabled) {
    cron.schedule(CRAWLER_CONFIG.github.interval, crawlGitHub);
    console.log('[Cron] GitHub crawler scheduled');
  }

  if (CRAWLER_CONFIG.forum.enabled) {
    cron.schedule(CRAWLER_CONFIG.forum.interval, crawlForums);
    console.log('[Cron] Forum crawler scheduled');
  }

  if (CRAWLER_CONFIG.validation.enabled) {
    cron.schedule(CRAWLER_CONFIG.validation.interval, validateLinks);
    console.log('[Cron] Link validator scheduled');
  }
}
```

#### 2. åœ¨åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–çˆ¬è™«

åœ¨ `server/index.ts` ä¸­æ·»åŠ ï¼š

```typescript
import { initializeCrawlerTasks } from './cron/crawler';

// ... å…¶ä»–ä»£ç  ...

async function startServer() {
  // ... å…¶ä»–åˆå§‹åŒ–ä»£ç  ...

  // åˆå§‹åŒ–çˆ¬è™«å®šæ—¶ä»»åŠ¡
  if (process.env.NODE_ENV === 'production') {
    initializeCrawlerTasks();
    console.log('[Server] Crawler tasks initialized');
  }

  // ... å¯åŠ¨æœåŠ¡å™¨ ...
}
```

#### 3. å®‰è£…ä¾èµ–

```bash
pnpm add node-cron
```

#### 4. é…ç½®ç¯å¢ƒå˜é‡

åœ¨ `.env` ä¸­æ·»åŠ ï¼š

```env
CRAWLER_ENABLED=true
CRAWLER_GITHUB_ENABLED=true
CRAWLER_GITHUB_MAX_STARS=100
CRAWLER_GITHUB_MAX_FORKS=50
CRAWLER_FORUM_ENABLED=true
CRAWLER_VALIDATION_ENABLED=true
CRAWLER_VALIDATION_INTERVAL=604800
```

---

## ğŸ”„ éƒ¨ç½²æ–¹å¼ 2ï¼šGitHub Actions å·¥ä½œæµ

### ä¼˜ç‚¹
- âœ… å…è´¹ï¼ˆGitHub æä¾› 2000 åˆ†é’Ÿ/æœˆï¼‰
- âœ… ä¸ GitHub é›†æˆ
- âœ… æ˜“äºç‰ˆæœ¬æ§åˆ¶
- âœ… è‡ªåŠ¨æ—¥å¿—è®°å½•

### å®ç°æ­¥éª¤

#### 1. åˆ›å»ºå·¥ä½œæµæ–‡ä»¶

åœ¨ `.github/workflows/crawler.yml` ä¸­åˆ›å»ºï¼š

```yaml
name: Subscription Links Crawler

on:
  schedule:
    # GitHub çˆ¬è™« - æ¯ 6 å°æ—¶æ‰§è¡Œä¸€æ¬¡
    - cron: '0 */6 * * *'
    # è®ºå›çˆ¬è™« - æ¯ 12 å°æ—¶æ‰§è¡Œä¸€æ¬¡
    - cron: '0 */12 * * *'
    # é“¾æ¥éªŒè¯ - æ¯å‘¨æ—¥åˆå¤œæ‰§è¡Œ
    - cron: '0 0 * * 0'
  workflow_dispatch: # å…è®¸æ‰‹åŠ¨è§¦å‘

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'pnpm'

      - name: Install pnpm
        uses: pnpm/action-setup@v2
        with:
          version: 10

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Run crawler
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: pnpm run crawler

      - name: Commit and push changes
        if: success()
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add .
          git commit -m "chore: update subscription links from crawler" || true
          git push

      - name: Send notification
        if: always()
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'Crawler job completed'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
        continue-on-error: true
```

#### 2. åˆ›å»ºçˆ¬è™«è„šæœ¬

åœ¨ `scripts/crawler.mjs` ä¸­åˆ›å»ºï¼š

```javascript
import axios from 'axios';
import fs from 'fs';
import path from 'path';

const GITHUB_API_URL = 'https://api.github.com';
const KEYWORDS = [
  'subscription',
  'proxy',
  'clash',
  'v2ray',
  'trojan',
  'vless',
  'vmess',
];

async function crawlGitHub() {
  console.log('Starting GitHub crawler...');

  const links = [];

  for (const keyword of KEYWORDS) {
    try {
      const query = `${keyword} in:readme stars:<100 forks:<50`;
      const url = `${GITHUB_API_URL}/search/repositories?q=${encodeURIComponent(query)}&sort=updated&order=desc&per_page=30`;

      const response = await axios.get(url, {
        headers: {
          Authorization: `token ${process.env.GITHUB_TOKEN}`,
          'User-Agent': 'SubLinksHub-Crawler',
        },
        timeout: 15000,
      });

      const repos = response.data.items || [];

      for (const repo of repos) {
        console.log(`Processing ${repo.full_name}...`);

        try {
          // è·å– README
          const readmeUrl = `https://raw.githubusercontent.com/${repo.full_name}/main/README.md`;
          const readmeResponse = await axios.get(readmeUrl, { timeout: 10000 });
          const content = readmeResponse.data;

          // æå–é“¾æ¥
          const linkRegex = /(https?:\/\/[^\s]+|vmess:\/\/[^\s]+|vless:\/\/[^\s]+|trojan:\/\/[^\s]+|ss:\/\/[^\s]+|ssr:\/\/[^\s]+)/gi;
          const matches = content.match(linkRegex) || [];

          for (const link of matches) {
            links.push({
              url: link.trim(),
              source: 'github',
              sourceUrl: repo.html_url,
              sourceTitle: repo.full_name,
              protocol: extractProtocol(link),
              stability: 'medium',
              tags: ['github', ...repo.topics],
            });
          }
        } catch (error) {
          console.error(`Error processing ${repo.full_name}:`, error.message);
        }
      }

      // é¿å… API é™åˆ¶
      await new Promise(resolve => setTimeout(resolve, 1000));
    } catch (error) {
      console.error(`Error searching for "${keyword}":`, error.message);
    }
  }

  return links;
}

function extractProtocol(url) {
  const match = url.match(/^([a-z]+):\/\//i);
  return match ? match[1].toLowerCase() : 'unknown';
}

async function main() {
  try {
    const links = await crawlGitHub();

    // ä¿å­˜ç»“æœ
    const outputPath = path.join(process.cwd(), 'crawler-results.json');
    fs.writeFileSync(outputPath, JSON.stringify(links, null, 2));

    console.log(`Crawler completed. Found ${links.length} links.`);
  } catch (error) {
    console.error('Crawler error:', error);
    process.exit(1);
  }
}

main();
```

#### 3. åœ¨ package.json ä¸­æ·»åŠ è„šæœ¬

```json
{
  "scripts": {
    "crawler": "node scripts/crawler.mjs"
  }
}
```

---

## ğŸŒ éƒ¨ç½²æ–¹å¼ 3ï¼šå¤–éƒ¨æœåŠ¡ï¼ˆRailway / Herokuï¼‰

### ä¼˜ç‚¹
- âœ… å®Œå…¨ç‹¬ç«‹
- âœ… é«˜åº¦å¯å®šåˆ¶
- âœ… æ”¯æŒé•¿æ—¶é—´è¿è¡Œ
- âœ… å¯æ‰©å±•æ€§å¼º

### å®ç°æ­¥éª¤ï¼ˆRailway ä¸ºä¾‹ï¼‰

#### 1. åˆ›å»ºç‹¬ç«‹çš„çˆ¬è™«åº”ç”¨

åœ¨ `crawler-app/` ç›®å½•ä¸‹åˆ›å»ºç‹¬ç«‹çš„ Node.js åº”ç”¨ï¼š

```
crawler-app/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ crawlers/
â”‚   â”‚   â”œâ”€â”€ github.js
â”‚   â”‚   â”œâ”€â”€ forum.js
â”‚   â”‚   â””â”€â”€ pastebin.js
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ db.js
â”‚   â”‚   â””â”€â”€ logger.js
â”‚   â””â”€â”€ index.js
â”œâ”€â”€ package.json
â”œâ”€â”€ .env.example
â””â”€â”€ Dockerfile
```

#### 2. åˆ›å»º Dockerfile

```dockerfile
FROM node:18-alpine

WORKDIR /app

COPY package*.json ./
RUN npm ci --only=production

COPY . .

CMD ["node", "src/index.js"]
```

#### 3. åˆ›å»ºçˆ¬è™«åº”ç”¨å…¥å£

åœ¨ `crawler-app/src/index.js` ä¸­ï¼š

```javascript
import cron from 'node-cron';
import axios from 'axios';
import { connectDB, saveLink } from './utils/db.js';
import { logger } from './utils/logger.js';

const config = {
  apiUrl: process.env.API_URL || 'http://localhost:3000',
  dbUrl: process.env.DATABASE_URL,
  crawlInterval: process.env.CRAWL_INTERVAL || '0 */6 * * *', // æ¯ 6 å°æ—¶
};

async function runCrawler() {
  try {
    logger.info('Crawler started');

    // è¿æ¥æ•°æ®åº“
    const db = await connectDB(config.dbUrl);

    // æ‰§è¡Œçˆ¬è™«
    const links = await crawlGitHub();
    logger.info(`Found ${links.length} links`);

    // ä¿å­˜é“¾æ¥
    for (const link of links) {
      await saveLink(db, link);
    }

    logger.info('Crawler completed successfully');
  } catch (error) {
    logger.error('Crawler error:', error);
  }
}

// è°ƒåº¦çˆ¬è™«
cron.schedule(config.crawlInterval, runCrawler);

logger.info(`Crawler scheduled with interval: ${config.crawlInterval}`);

// å¯åŠ¨æœåŠ¡å™¨ï¼ˆç”¨äºå¥åº·æ£€æŸ¥ï¼‰
import express from 'express';
const app = express();

app.get('/health', (req, res) => {
  res.json({ status: 'ok' });
});

app.listen(3001, () => {
  logger.info('Health check server listening on port 3001');
});
```

#### 4. éƒ¨ç½²åˆ° Railway

```bash
# å®‰è£… Railway CLI
npm install -g @railway/cli

# ç™»å½• Railway
railway login

# åˆå§‹åŒ–é¡¹ç›®
railway init

# éƒ¨ç½²
railway up
```

#### 5. é…ç½®ç¯å¢ƒå˜é‡

åœ¨ Railway ä»ªè¡¨æ¿ä¸­è®¾ç½®ï¼š

```
DATABASE_URL=your_database_url
API_URL=https://your-app.manus.space
GITHUB_TOKEN=your_github_token
CRAWL_INTERVAL=0 */6 * * *
```

---

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

### æ—¥å¿—ä½ç½®

| éƒ¨ç½²æ–¹å¼ | æ—¥å¿—ä½ç½® |
| :--- | :--- |
| Manus | åº”ç”¨æ—¥å¿— / æ§åˆ¶å° |
| GitHub Actions | Actions é€‰é¡¹å¡ / å·¥ä½œæµæ—¥å¿— |
| Railway | Railway ä»ªè¡¨æ¿ / æ—¥å¿—æ ‡ç­¾ |

### ç›‘æ§æŒ‡æ ‡

- çˆ¬è™«æ‰§è¡Œæ—¶é—´
- å‘ç°çš„æ–°é“¾æ¥æ•°
- å¤±è´¥ç‡
- æ•°æ®åº“å†™å…¥é€Ÿåº¦

---

## ğŸ” å®‰å…¨å»ºè®®

1. **API å¯†é’¥ç®¡ç†**
   - ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨æ•æ„Ÿä¿¡æ¯
   - å®šæœŸè½®æ¢å¯†é’¥
   - ä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç å¯†é’¥

2. **é€Ÿç‡é™åˆ¶**
   - éµå®ˆ API é€Ÿç‡é™åˆ¶
   - å®ç°æŒ‡æ•°é€€é¿é‡è¯•
   - ä½¿ç”¨ä»£ç†æ± é¿å… IP è¢«å°

3. **æ•°æ®éªŒè¯**
   - éªŒè¯æ‰€æœ‰çˆ¬è™«æ•°æ®
   - æ£€æŸ¥é“¾æ¥æ ¼å¼
   - è¿‡æ»¤æ¶æ„å†…å®¹

4. **ç›‘æ§å’Œå‘Šè­¦**
   - ç›‘æ§çˆ¬è™«æ‰§è¡ŒçŠ¶æ€
   - è®¾ç½®å¤±è´¥å‘Šè­¦
   - è®°å½•æ‰€æœ‰å¼‚å¸¸

---

## ğŸš¨ æ•…éšœæ’é™¤

### é—®é¢˜ 1ï¼šçˆ¬è™«è¶…æ—¶

**è§£å†³æ–¹æ¡ˆï¼š**
- å¢åŠ è¶…æ—¶æ—¶é—´
- å‡å°‘å¹¶å‘è¯·æ±‚æ•°
- æ£€æŸ¥ç½‘ç»œè¿æ¥

### é—®é¢˜ 2ï¼šAPI é™åˆ¶

**è§£å†³æ–¹æ¡ˆï¼š**
- ä½¿ç”¨ Personal Access Token
- å®ç°ç¼“å­˜æœºåˆ¶
- å‡å°‘è¯·æ±‚é¢‘ç‡

### é—®é¢˜ 3ï¼šæ•°æ®åº“è¿æ¥å¤±è´¥

**è§£å†³æ–¹æ¡ˆï¼š**
- æ£€æŸ¥è¿æ¥å­—ç¬¦ä¸²
- éªŒè¯æ•°æ®åº“å‡­è¯
- æ£€æŸ¥é˜²ç«å¢™è§„åˆ™

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

1. **å¹¶å‘çˆ¬è™«**
   ```javascript
   const results = await Promise.all(crawlers.map(c => c.run()));
   ```

2. **ç¼“å­˜æœºåˆ¶**
   ```javascript
   const cache = new Map();
   if (cache.has(url)) return cache.get(url);
   ```

3. **æ‰¹é‡æ•°æ®åº“æ“ä½œ**
   ```javascript
   await db.insertMany(links); // è€Œä¸æ˜¯é€ä¸ªæ’å…¥
   ```

---

## ğŸ“š å‚è€ƒèµ„æº

- [GitHub API æ–‡æ¡£](https://docs.github.com/en/rest)
- [node-cron æ–‡æ¡£](https://www.npmjs.com/package/node-cron)
- [Railway æ–‡æ¡£](https://docs.railway.app)
- [GitHub Actions æ–‡æ¡£](https://docs.github.com/en/actions)

---

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æäº¤ Issue æˆ–è”ç³»å¼€å‘å›¢é˜Ÿã€‚
